import os
import logging
from typing import List, Dict
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_core.documents import Document

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö Vector DB (‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô Folder ‡∏ä‡∏∑‡πà‡∏≠ 'chroma_db' ‡πÉ‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå)
PERSIST_DIRECTORY = os.path.join(os.getcwd(), "chroma_db")

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Embedding Model (‡πÉ‡∏ä‡πâ Ollama: nomic-embed-text)
embeddings = OllamaEmbeddings(
    model="nomic-embed-text",
    base_url="http://localhost:11434"
)

# ‡πÇ‡∏´‡∏•‡∏î Vector DB ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
vector_db = Chroma(
    collection_name="jira_knowledge",
    embedding_function=embeddings,
    persist_directory=PERSIST_DIRECTORY
)


def add_ticket_to_vector(ticket_data: Dict):
    """
    ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Ticket ‡πÄ‡∏õ‡πá‡∏ô Vector ‡πÅ‡∏•‡πâ‡∏ß‡∏¢‡∏±‡∏î‡∏•‡∏á DB
    Ticket Data ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ: key, summary, status, logic, spec
    """
    # 1. ‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: ‡∏™‡∏£‡πâ‡∏≤‡∏á Text ‡∏Å‡πâ‡∏≠‡∏ô‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
    # ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏™‡πà‡∏ß‡∏ô 'Details' ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ñ‡∏≤‡∏°‡∏ñ‡∏∂‡∏á‡∏Ñ‡∏£‡∏±‡∏ö
    page_content = f"""
    TICKET: {ticket_data.get('key')}
    SUMMARY: {ticket_data.get('summary')}
    STATUS: {ticket_data.get('status')}

    [BUSINESS LOGIC]
    {ticket_data.get('logic') or 'N/A'}

    [TECHNICAL SPEC]
    {ticket_data.get('spec') or 'N/A'}
    """

    # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á Metadata (‡πÄ‡∏≠‡∏≤‡πÑ‡∏ß‡πâ filter ‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á‡πÑ‡∏î‡πâ)
    metadata = {
        "key": ticket_data.get('key'),
        "status": ticket_data.get('status'),
        "type": "jira_ticket"
    }

    # 3. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á ChromaDB (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏±‡∏ö‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡∏ï‡πâ‡∏≠‡∏á‡∏•‡∏ö‡∏Å‡πà‡∏≠‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ‡∏°‡∏±‡∏ô‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ ID ‡πÄ‡∏≠‡∏á)
    # ‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ key ‡πÄ‡∏õ‡πá‡∏ô id ‡∏Ç‡∏≠‡∏á vector document ‡πÄ‡∏•‡∏¢ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô‡∏ã‡πâ‡∏≥
    logging.info(f"üß≤ Vectorizing {ticket_data.get('key')}...")
    vector_db.add_documents(
        documents=[Document(page_content=page_content, metadata=metadata)],
        ids=[ticket_data.get('key')]
    )


def search_vector_db(query: str, k: int = 4):
    """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ (Semantic Search)"""
    logging.info(f"üß† Semantic Searching for: '{query}'")

    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ k ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
    results = vector_db.similarity_search_with_score(query, k=k)

    parsed_results = []
    for doc, score in results:
        parsed_results.append(f"""
        --- MATCH (Score: {score:.2f}) ---
        {doc.page_content}
        -----------------------------------
        """)

    return "\n".join(parsed_results)