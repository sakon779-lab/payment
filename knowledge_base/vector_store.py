import os
import logging
from typing import List, Dict
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_core.documents import Document

# üëá FIX: ‡πÉ‡∏ä‡πâ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏ñ‡∏≠‡∏¢‡∏´‡∏•‡∏±‡∏á‡∏°‡∏≤ 1 step ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ Project Root
CURRENT_FILE_PATH = os.path.abspath(__file__) # D:\Project\PaymentBlockChain\knowledge_base\vector_store.py
BASE_DIR = os.path.dirname(os.path.dirname(CURRENT_FILE_PATH)) # D:\Project\PaymentBlockChain

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö Vector DB (‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô Folder ‡∏ä‡∏∑‡πà‡∏≠ 'chroma_db' ‡πÉ‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå)
PERSIST_DIRECTORY = os.path.join(BASE_DIR, "chroma_db")

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Embedding Model (‡πÉ‡∏ä‡πâ Ollama: nomic-embed-text)
embeddings = OllamaEmbeddings(
    model="nomic-embed-text",
    base_url="http://localhost:11434"
)

# ‡πÇ‡∏´‡∏•‡∏î Vector DB ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
vector_db = Chroma(
    collection_name="jira_knowledge",
    embedding_function=embeddings,
    persist_directory=PERSIST_DIRECTORY
)

# üëá ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏±‡∏ö 3 ‡∏Ñ‡πà‡∏≤‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏Ñ‡∏£‡∏±‡∏ö
def add_ticket_to_vector(issue_key: str, summary: str, content: str):
    """
    Save ticket data to Vector DB for semantic search.
    """
    logging.info(f"üß† VECTOR: Embedding ticket {issue_key}...")

    full_text = f"""
    Ticket: {issue_key}
    Summary: {summary}
    Details: {content}
    """

    doc = Document(
        page_content=full_text,
        metadata={"issue_key": issue_key, "source": "jira"}
    )

    # ‡∏•‡∏ö‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà
    try:
        existing = vector_db.get(where={"issue_key": issue_key})
        if existing and existing['ids']:
            vector_db.delete(ids=existing['ids'])
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è Vector delete error (ignorable): {e}")

    vector_db.add_documents([doc])
    logging.info(f"‚úÖ VECTOR: Saved {issue_key} successfully.")


def search_vector_db(query: str, k: int = 4):
    """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ (Semantic Search)"""
    logging.info(f"üß† Semantic Searching for: '{query}'")

    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ k ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
    results = vector_db.similarity_search_with_score(query, k=k)

    parsed_results = []
    for doc, score in results:
        parsed_results.append(f"""
        --- MATCH (Score: {score:.2f}) ---
        {doc.page_content}
        -----------------------------------
        """)

    return "\n".join(parsed_results)